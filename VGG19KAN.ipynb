{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9643857,"sourceType":"datasetVersion","datasetId":5889380}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms, models\nfrom torch.utils.data import DataLoader\nimport torch.nn.functional as F\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nimport math\nimport time\nimport matplotlib.pyplot as plt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-08T06:42:00.158887Z","iopub.execute_input":"2024-11-08T06:42:00.159723Z","iopub.status.idle":"2024-11-08T06:42:05.384083Z","shell.execute_reply.started":"2024-11-08T06:42:00.159681Z","shell.execute_reply":"2024-11-08T06:42:05.383038Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Check for GPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# 1. Define image transformations for training, validation, and test sets\ntrain_transform = transforms.Compose([\n    transforms.Resize((224, 224)),                     # Resize to 224x224\n    transforms.ToTensor(),                              # Convert to tensor\n    transforms.RandomHorizontalFlip(),                 # Random horizontal flip\n    transforms.RandomRotation(90),                     # Random rotation up to 90 degrees\n    transforms.ColorJitter(brightness=0.2, contrast=0.2),  # Adjust brightness/contrast\n    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),  # Random crop and resize\n    transforms.RandomErasing(p=0.1)                   # Random erasing (similar to dropout)\n])\n\nval_transform = transforms.Compose([\n    transforms.Resize((224, 224)),  # Resize for validation\n    transforms.ToTensor()\n])\n\ntest_transform = transforms.Compose([\n    transforms.Resize((224, 224)),  # Resize for testing\n    transforms.ToTensor()\n])","metadata":{"execution":{"iopub.status.busy":"2024-11-08T06:43:44.099435Z","iopub.execute_input":"2024-11-08T06:43:44.100831Z","iopub.status.idle":"2024-11-08T06:43:44.140473Z","shell.execute_reply.started":"2024-11-08T06:43:44.100759Z","shell.execute_reply":"2024-11-08T06:43:44.139446Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# 2. Load datasets using ImageFolder\ntrain_dataset = datasets.ImageFolder(root='path to train dataset', transform=train_transform)\nval_dataset = datasets.ImageFolder(root='path to validation dataset', transform=val_transform)\ntest_dataset = datasets.ImageFolder(root='path to test dataset', transform=test_transform)","metadata":{"execution":{"iopub.status.busy":"2024-11-08T06:43:47.403401Z","iopub.execute_input":"2024-11-08T06:43:47.404155Z","iopub.status.idle":"2024-11-08T06:43:50.247737Z","shell.execute_reply.started":"2024-11-08T06:43:47.404117Z","shell.execute_reply":"2024-11-08T06:43:50.246750Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# 3. Create DataLoaders for train, val, and test datasets\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\nval_loadere = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-08T06:43:53.620765Z","iopub.execute_input":"2024-11-08T06:43:53.621668Z","iopub.status.idle":"2024-11-08T06:43:53.627068Z","shell.execute_reply.started":"2024-11-08T06:43:53.621628Z","shell.execute_reply":"2024-11-08T06:43:53.626021Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":"#don't run for now\ndef run(model, criterion, optimizer, train_loader_rice, test_loader_rice):\n    # Lists to store the metrics\n    train_accuracies = []\n    train_losses = []\n    val_accuracies = []\n    val_losses = []\n    num_epochs = 80\n    \n    for epoch in range(num_epochs):\n        # Training phase\n        model.train()\n        running_loss = 0.0\n        total_train = 0\n        correct_train = 0\n        all_train_labels = []\n        all_train_preds = []\n\n        for images, labels in train_loader_rice:\n            images, labels = images.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n\n            _, predicted = torch.max(outputs, 1)\n            total_train += labels.size(0)\n            correct_train += (predicted == labels).sum().item()\n\n            all_train_labels.extend(labels.cpu().numpy())\n            all_train_preds.extend(predicted.cpu().numpy())\n\n        train_precision = precision_score(all_train_labels, all_train_preds, average='macro', zero_division=0)\n        train_recall = recall_score(all_train_labels, all_train_preds, average='macro')\n        train_f1 = f1_score(all_train_labels, all_train_preds, average='macro')\n        train_accuracy = 100 * correct_train / total_train\n        train_loss = running_loss / len(train_loader_rice)\n        train_accuracies.append(train_accuracy)\n        train_losses.append(train_loss)\n\n        # Validation phase\n        model.eval()\n        val_loss = 0.0\n        total_val = 0\n        correct_val = 0\n        all_val_labels = []\n        all_val_preds = []\n\n        # Inference time measurement on the test set\n        total_inference_time = 0.0\n\n        with torch.no_grad():\n            for images, labels in test_loader_rice:  # Using test_loader here\n                images, labels = images.to(device), labels.to(device)\n\n                # Start timing before inference\n                start_time = time.time()\n\n                outputs = model(images)\n\n                # End timing after inference\n                end_time = time.time()\n\n                # Add the time taken for this batch to total inference time\n                total_inference_time += end_time - start_time\n\n                loss = criterion(outputs, labels)\n                val_loss += loss.item()\n\n                _, predicted = torch.max(outputs, 1)\n                total_val += labels.size(0)\n                correct_val += (predicted == labels).sum().item()\n\n                all_val_labels.extend(labels.cpu().numpy())\n                all_val_preds.extend(predicted.cpu().numpy())\n\n        val_precision = precision_score(all_val_labels, all_val_preds, average='macro', zero_division=0)\n        val_recall = recall_score(all_val_labels, all_val_preds, average='macro')\n        val_f1 = f1_score(all_val_labels, all_val_preds, average='macro')\n        val_accuracy = 100 * correct_val / total_val\n        val_loss /= len(test_loader_rice)\n        val_accuracies.append(val_accuracy)\n        val_losses.append(val_loss)\n\n        total_images = len(test_loader_rice.dataset)\n        avg_inference_time = total_inference_time / total_images\n\n        print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n              f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, \"\n              f\"Train Precision: {train_precision:.2f}, Train Recall: {train_recall:.2f}, Train F1: {train_f1:.2f}, \"\n              f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%, \"\n              f\"Val Precision: {val_precision:.2f}, Val Recall: {val_recall:.2f}, Val F1: {val_f1:.2f}, \"\n              f\"Avg Inference Time per Image: {avg_inference_time:.6f} seconds\")\n\n    # Return the metrics for plotting\n    return train_accuracies, train_losses, val_accuracies, val_losses\n","metadata":{"execution":{"iopub.status.busy":"2024-10-24T08:59:45.840506Z","iopub.execute_input":"2024-10-24T08:59:45.841478Z","iopub.status.idle":"2024-10-24T08:59:45.859480Z","shell.execute_reply.started":"2024-10-24T08:59:45.841434Z","shell.execute_reply":"2024-10-24T08:59:45.858302Z"},"trusted":true},"outputs":[],"execution_count":27},{"cell_type":"code","source":"def run(model, criterion, optimizer, train_loader, val_loader, test_loader):\n    train_accuracies = []\n    val_accuracies = []\n    test_accuracies = []\n    train_losses = []\n    val_losses = []\n    test_losses = []\n    test_precisions = []\n    test_recalls = []\n    test_f1_scores = []\n    num_epochs = 80\n\n    for epoch in range(num_epochs):\n        # Training phase\n        model.train()\n        running_train_loss = 0.0\n        total_train = 0\n        correct_train = 0\n\n        for images, labels in train_loader:\n            images, labels = images.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            running_train_loss += loss.item() * images.size(0)\n            _, predicted = torch.max(outputs, 1)\n            total_train += labels.size(0)\n            correct_train += (predicted == labels).sum().item()\n\n        train_accuracy = 100 * correct_train / total_train\n        train_loss = running_train_loss / total_train\n        train_accuracies.append(train_accuracy)\n        train_losses.append(train_loss)\n\n        # Validation phase\n        model.eval()\n        running_val_loss = 0.0\n        total_val = 0\n        correct_val = 0\n\n        with torch.no_grad():\n            for images, labels in val_loader:\n                images, labels = images.to(device), labels.to(device)\n                outputs = model(images)\n                loss = criterion(outputs, labels)\n                running_val_loss += loss.item() * images.size(0)\n                _, predicted = torch.max(outputs, 1)\n                total_val += labels.size(0)\n                correct_val += (predicted == labels).sum().item()\n\n        val_accuracy = 100 * correct_val / total_val\n        val_loss = running_val_loss / total_val\n        val_accuracies.append(val_accuracy)\n        val_losses.append(val_loss)\n\n        # Test phase\n        running_test_loss = 0.0\n        all_preds = []\n        all_labels = []\n\n        with torch.no_grad():\n            for images, labels in test_loader:\n                images, labels = images.to(device), labels.to(device)\n                outputs = model(images)\n                loss = criterion(outputs, labels)\n                running_test_loss += loss.item() * images.size(0)\n                _, predicted = torch.max(outputs, 1)\n                all_preds.extend(predicted.cpu().numpy())\n                all_labels.extend(labels.cpu().numpy())\n\n        test_accuracy = 100 * sum(p == t for p, t in zip(all_preds, all_labels)) / len(all_labels)\n        test_loss = running_test_loss / len(test_loader.dataset)\n        test_accuracies.append(test_accuracy)\n        test_losses.append(test_loss)\n\n        precision = precision_score(all_labels, all_preds, average=\"weighted\")\n        recall = recall_score(all_labels, all_preds, average=\"weighted\")\n        f1 = f1_score(all_labels, all_preds, average=\"weighted\")\n        \n        test_precisions.append(precision)\n        test_recalls.append(recall)\n        test_f1_scores.append(f1)\n\n        print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n              f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, \"\n              f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%, \"\n              f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%, \"\n              f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}\")\n\n    return train_accuracies, val_accuracies, test_accuracies, train_losses, val_losses, test_losses, test_precisions, test_recalls, test_f1_scores","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-08T06:43:57.688619Z","iopub.execute_input":"2024-11-08T06:43:57.689012Z","iopub.status.idle":"2024-11-08T06:43:57.707208Z","shell.execute_reply.started":"2024-11-08T06:43:57.688975Z","shell.execute_reply":"2024-11-08T06:43:57.706111Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# KANLinear definition Soure: https://github.com/Blealtan/efficient-kan/blob/f39e5146af34299ad3a581d2106eb667ba0fa6fa/src/efficient_kan/kan.py#L6\nclass KANLinear(torch.nn.Module):\n    def __init__(\n        self,\n        in_features,\n        out_features,\n        grid_size=5,\n        spline_order=3,\n        scale_noise=0.1,\n        scale_base=1.0,\n        scale_spline=1.0,\n        enable_standalone_scale_spline=True,\n        base_activation=torch.nn.SiLU,\n        grid_eps=0.02,\n        grid_range=[-1, 1],\n    ):\n        super(KANLinear, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.grid_size = grid_size\n        self.spline_order = spline_order\n\n        h = (grid_range[1] - grid_range[0]) / grid_size\n        grid = (\n            (\n                torch.arange(-spline_order, grid_size + spline_order + 1) * h\n                + grid_range[0]\n            )\n            .expand(in_features, -1)\n            .contiguous()\n        )\n        self.register_buffer(\"grid\", grid)\n\n        self.base_weight = torch.nn.Parameter(torch.Tensor(out_features, in_features))\n        self.spline_weight = torch.nn.Parameter(\n            torch.Tensor(out_features, in_features, grid_size + spline_order)\n        )\n        if enable_standalone_scale_spline:\n            self.spline_scaler = torch.nn.Parameter(\n                torch.Tensor(out_features, in_features)\n            )\n\n        self.scale_noise = scale_noise\n        self.scale_base = scale_base\n        self.scale_spline = scale_spline\n        self.enable_standalone_scale_spline = enable_standalone_scale_spline\n        self.base_activation = base_activation()\n        self.grid_eps = grid_eps\n\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        torch.nn.init.kaiming_uniform_(self.base_weight, a=math.sqrt(5) * self.scale_base)\n        with torch.no_grad():\n            noise = (\n                (\n                    torch.rand(self.grid_size + 1, self.in_features, self.out_features)\n                    - 1 / 2\n                )\n                * self.scale_noise\n                / self.grid_size\n            )\n            self.spline_weight.data.copy_(\n                (self.scale_spline if not self.enable_standalone_scale_spline else 1.0)\n                * self.curve2coeff(\n                    self.grid.T[self.spline_order : -self.spline_order],\n                    noise,\n                )\n            )\n            if self.enable_standalone_scale_spline:\n                # torch.nn.init.constant_(self.spline_scaler, self.scale_spline)\n                torch.nn.init.kaiming_uniform_(self.spline_scaler, a=math.sqrt(5) * self.scale_spline)\n\n    def b_splines(self, x: torch.Tensor):\n        \"\"\"\n        Compute the B-spline bases for the given input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n\n        Returns:\n            torch.Tensor: B-spline bases tensor of shape (batch_size, in_features, grid_size + spline_order).\n        \"\"\"\n        assert x.dim() == 2 and x.size(1) == self.in_features\n\n        grid: torch.Tensor = (\n            self.grid\n        )  # (in_features, grid_size + 2 * spline_order + 1)\n        x = x.unsqueeze(-1)\n        bases = ((x >= grid[:, :-1]) & (x < grid[:, 1:])).to(x.dtype)\n        for k in range(1, self.spline_order + 1):\n            bases = (\n                (x - grid[:, : -(k + 1)])\n                / (grid[:, k:-1] - grid[:, : -(k + 1)])\n                * bases[:, :, :-1]\n            ) + (\n                (grid[:, k + 1 :] - x)\n                / (grid[:, k + 1 :] - grid[:, 1:(-k)])\n                * bases[:, :, 1:]\n            )\n\n        assert bases.size() == (\n            x.size(0),\n            self.in_features,\n            self.grid_size + self.spline_order,\n        )\n        return bases.contiguous()\n\n    def curve2coeff(self, x: torch.Tensor, y: torch.Tensor):\n        \"\"\"\n        Compute the coefficients of the curve that interpolates the given points.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n            y (torch.Tensor): Output tensor of shape (batch_size, in_features, out_features).\n\n        Returns:\n            torch.Tensor: Coefficients tensor of shape (out_features, in_features, grid_size + spline_order).\n        \"\"\"\n        assert x.dim() == 2 and x.size(1) == self.in_features\n        assert y.size() == (x.size(0), self.in_features, self.out_features)\n\n        A = self.b_splines(x).transpose(\n            0, 1\n        )  # (in_features, batch_size, grid_size + spline_order)\n        B = y.transpose(0, 1)  # (in_features, batch_size, out_features)\n        solution = torch.linalg.lstsq(\n            A, B\n        ).solution  # (in_features, grid_size + spline_order, out_features)\n        result = solution.permute(\n            2, 0, 1\n        )  # (out_features, in_features, grid_size + spline_order)\n\n        assert result.size() == (\n            self.out_features,\n            self.in_features,\n            self.grid_size + self.spline_order,\n        )\n        return result.contiguous()\n\n    @property\n    def scaled_spline_weight(self):\n        return self.spline_weight * (\n            self.spline_scaler.unsqueeze(-1)\n            if self.enable_standalone_scale_spline\n            else 1.0\n        )\n\n    def forward(self, x: torch.Tensor):\n        assert x.dim() == 2 and x.size(1) == self.in_features\n\n        base_output = F.linear(self.base_activation(x), self.base_weight)\n        spline_output = F.linear(\n            self.b_splines(x).view(x.size(0), -1),\n            self.scaled_spline_weight.view(self.out_features, -1),\n        )\n        return base_output + spline_output\n\n    @torch.no_grad()\n    def update_grid(self, x: torch.Tensor, margin=0.01):\n        assert x.dim() == 2 and x.size(1) == self.in_features\n        batch = x.size(0)\n\n        splines = self.b_splines(x)  # (batch, in, coeff)\n        splines = splines.permute(1, 0, 2)  # (in, batch, coeff)\n        orig_coeff = self.scaled_spline_weight  # (out, in, coeff)\n        orig_coeff = orig_coeff.permute(1, 2, 0)  # (in, coeff, out)\n        unreduced_spline_output = torch.bmm(splines, orig_coeff)  # (in, batch, out)\n        unreduced_spline_output = unreduced_spline_output.permute(\n            1, 0, 2\n        )  # (batch, in, out)\n\n        # sort each channel individually to collect data distribution\n        x_sorted = torch.sort(x, dim=0)[0]\n        grid_adaptive = x_sorted[\n            torch.linspace(\n                0, batch - 1, self.grid_size + 1, dtype=torch.int64, device=x.device\n            )\n        ]\n\n        uniform_step = (x_sorted[-1] - x_sorted[0] + 2 * margin) / self.grid_size\n        grid_uniform = (\n            torch.arange(\n                self.grid_size + 1, dtype=torch.float32, device=x.device\n            ).unsqueeze(1)\n            * uniform_step\n            + x_sorted[0]\n            - margin\n        )\n\n        grid = self.grid_eps * grid_uniform + (1 - self.grid_eps) * grid_adaptive\n        grid = torch.concatenate(\n            [\n                grid[:1]\n                - uniform_step\n                * torch.arange(self.spline_order, 0, -1, device=x.device).unsqueeze(1),\n                grid,\n                grid[-1:]\n                + uniform_step\n                * torch.arange(1, self.spline_order + 1, device=x.device).unsqueeze(1),\n            ],\n            dim=0,\n        )\n\n        self.grid.copy_(grid.T)\n        self.spline_weight.data.copy_(self.curve2coeff(x, unreduced_spline_output))\n\n    def regularization_loss(self, regularize_activation=1.0, regularize_entropy=1.0):\n        \"\"\"\n        Compute the regularization loss.\n\n        This is a dumb simulation of the original L1 regularization as stated in the\n        paper, since the original one requires computing absolutes and entropy from the\n        expanded (batch, in_features, out_features) intermediate tensor, which is hidden\n        behind the F.linear function if we want an memory efficient implementation.\n\n        The L1 regularization is now computed as mean absolute value of the spline\n        weights. The authors implementation also includes this term in addition to the\n        sample-based regularization.\n        \"\"\"\n        l1_fake = self.spline_weight.abs().mean(-1)\n        regularization_loss_activation = l1_fake.sum()\n        p = l1_fake / regularization_loss_activation\n        regularization_loss_entropy = -torch.sum(p * p.log())\n        return (\n            regularize_activation * regularization_loss_activation\n            + regularize_entropy * regularization_loss_entropy\n        )\n","metadata":{"execution":{"iopub.status.busy":"2024-11-08T06:44:01.841305Z","iopub.execute_input":"2024-11-08T06:44:01.841687Z","iopub.status.idle":"2024-11-08T06:44:01.875479Z","shell.execute_reply.started":"2024-11-08T06:44:01.841649Z","shell.execute_reply":"2024-11-08T06:44:01.874603Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class VGG19KAN(nn.Module):\n    def __init__(self, num_classes):\n        super(VGG19KAN, self).__init__()\n        # Load pre-trained VGG19\n        vgg19 = models.vgg19(pretrained=True)\n        \n        # Use the features (convolutional layers) from the pre-trained VGG19\n        self.features = vgg19.features  # This includes the convolutional layers up to AdaptiveAvgPool2d\n        \n        # Adaptive average pooling layer to reduce the feature maps to a fixed size\n        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n        \n        # Define KANLinear layers to replace the original VGG19 fully connected layers\n        self.flattened_size = 512 * 7 * 7  # The output from the last VGG conv layer will be 512x7x7\n        self.kan1 = KANLinear(self.flattened_size, 512)\n        self.kan2 = KANLinear(512, 1024)\n        self.kan3 = KANLinear(1024, num_classes)\n\n    def forward(self, x):\n        # Pass through VGG19 feature extractor\n        x = self.features(x)\n        \n        # Adaptive average pooling\n        x = self.avgpool(x)\n        \n        # Flatten the output\n        x = x.view(x.size(0), -1)\n        \n        # Pass through KANLinear layers\n        x = self.kan1(x)\n        x = self.kan2(x)\n        x = self.kan3(x)\n        \n        return x","metadata":{"execution":{"iopub.status.busy":"2024-11-08T06:44:11.679805Z","iopub.execute_input":"2024-11-08T06:44:11.680617Z","iopub.status.idle":"2024-11-08T06:44:11.688568Z","shell.execute_reply.started":"2024-11-08T06:44:11.680579Z","shell.execute_reply":"2024-11-08T06:44:11.687624Z"},"trusted":true},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Model, loss function, and optimizer\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel1 = VGG19KAN(num_classes=len(train_dataset_rice.classes)).to(device)\ncriterion1 = nn.CrossEntropyLoss()\noptimizer1 = optim.AdamW(model1.parameters(), lr=0.0001, weight_decay=1e-4)\ntrain_accuracies, val_accuracies, test_accuracies = run(model, criterion, optimizer, train_loader, val_loader, test_loader)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}